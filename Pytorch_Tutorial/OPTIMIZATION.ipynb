{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTikF4sEqt/1Gfm5ly2jUJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 모델 매개변수 최적화 하기\n","이제 모델고 데이터가 준비되었으니, 데이터에 매개변수를 최적화하여 모델을 학습하고, 검증하고, 테스트할 차례입니다.\n","모델을 학습하는 과정은 반복적인 과정을 거칩니다. 각 반복 단계에서 모델은 출력을 추측하고, 추측과 정답 사이의 오류(loss)를 계산하고, 매개변수에 대한 오류의 도함수(derivative)를 수집한 뒤, gradient descent를 사용하여 이 parameter들을 optimize 합니다.\n"],"metadata":{"id":"G4oKIkSEds27"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"O0xW8h8Mdnj2","executionInfo":{"status":"ok","timestamp":1691051050495,"user_tz":-540,"elapsed":388,"user":{"displayName":"안창준","userId":"13592319070657983406"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","training_data = datasets.FashionMNIST(\n","    root =\"./data\",\n","    train = True,\n","    download = True,\n","    transform = ToTensor()\n",")\n","test_data = datasets.FashionMNIST(\n","    root =\"./data\",\n","    train = False,\n","    download = True,\n","    transform = ToTensor()\n",")\n","\n","train_dataloader = DataLoader(training_data, batch_size = 64)\n","test_dataloader = DataLoader(test_data, batch_size = 64)\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28,512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()"]},{"cell_type":"code","source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"],"metadata":{"id":"QYjBKaBQfIN1","executionInfo":{"status":"ok","timestamp":1691051075455,"user_tz":-540,"elapsed":3,"user":{"displayName":"안창준","userId":"13592319070657983406"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 최적화 단계 (Optimization Loop)\n","하이퍼파라미터를 설정한 뒤에는 최적화 단게를 통해 모델을 학습하고 최적화 할수 있습니다. 최적화 단계의 각 반복(iteration)을 epoch라고 부릅니다.\n","하나의 에폭은 다음 두 부분으로 구성됩니다.\n","\n","\n","*   학습 단계(train loop) - 학습용 데이터셋을 반복(iterate)하고 최적의 매개변수로 수렴합니다.\n","*   검증/테스트 단계(validation/test loop) - 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복합니다.\n","\n","학습 단계(training loop)에서 일어나는 몇 가지 개념들을 간략히 살펴보겠습니디ㅏ.\n","\n","# 손실 함수\n","\n","학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높습니다. 손실 함수(loss function)는 획득한 결과와 실제 값 사이의 틀린 정도(degree of dissimilarity)를 측정하며, 학습 중에 이 값을 최소화하려고 합니다. 주어진 데이터 샘플을 입력으로 계산한 예측과 정답(label)을 비교하여 손실(loss)을 계산합니다.\n","\n","일반적인 손실함수에는 회귀 문제(regression task)에 사용하는 nn.MSELoss(평균 제곱 오차(MSE; Mean Square Error))나 분류(classification)에 사용하는 nn.NLLLoss (음의 로그 우도(Negative Log Likelihood)), 그리고 nn.LogSoftmax와 nn.NLLLoss를 합친 nn.CrossEntropyLoss 등이 있습니다.\n","\n","# optimizer\n","\n","최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정입니다. 최적화 알고리즘은 이 과정이 수행되는 방식(여기에서는 확률적 경사하강법(SGD; Stochastic Gradient Descent))을 정의합니다. 모든 최적화 절차(logic)는 optimizer 객체에 캡슐화(encapsulate)됩니다. 여기서는 SGD 옵티마이저를 사용하고 있으며, PyTorch에는 ADAM이나 RMSProp과 같은 다른 종류의 모델과 데이터에서 더 잘 동작하는 다양한 옵티마이저가 있습니다.\n","\n","학습하려는 모델의 매개변수와 학습률(learning rate) 하이퍼파라미터를 등록하여 옵티마이저를 초기화합니다.\n","\n","학습 단계(loop)에서는 최적화는 세단계로 이루어집니다.\n","\n","\n","*   optimizer.zero_grad() 를 호출하여 모델 매개변수의 변화도를 재설정합니다. 기본적으로 변화도는 더해지기 때문에 중복 계산을 막기 위해 반복할 때 마다 명시적으로 0으로 설정합니다.\n","*   loss.backwards()를 호출하여 예측 손실(prediction loss)를 역전파 합니다. PyTorch는 각 매개변수에 대한 손실의 변화도를 저장합니다.\n","*   변화도를 계산한 뒤에는 optimizer.step()을 호출하여 역전파 단게에서 수집된 변화도로 매개변수를 조정합니다.\n","\n"],"metadata":{"id":"JOo5Ldipf7QW"}},{"cell_type":"markdown","source":["# 전체구현"],"metadata":{"id":"c8Dzw1xmhVSe"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch  %100 == 0:\n","            loss, current = loss.item(), (batch + 1)*len(X)\n","            print(f\"loss: {loss:>7f} [{current:5>d}/{size:>5d}]\")\n","\n","def test_loop(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct =0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= num_batches\n","    correct /= size\n","\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n","epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xl6do15Vf6Oo","executionInfo":{"status":"ok","timestamp":1691051954385,"user_tz":-540,"elapsed":117033,"user":{"displayName":"안창준","userId":"13592319070657983406"}},"outputId":"844911c0-140b-47fe-8141-cd5c68cdf18f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","loss: 2.299218 [64/60000]\n","loss: 2.288791 [6464/60000]\n","loss: 2.274296 [12864/60000]\n","loss: 2.273421 [19264/60000]\n","loss: 2.245553 [25664/60000]\n","loss: 2.232956 [32064/60000]\n","loss: 2.228132 [38464/60000]\n","loss: 2.203117 [44864/60000]\n","loss: 2.207247 [51264/60000]\n","loss: 2.168816 [57664/60000]\n","Test Error: \n"," Accuracy: 40.7%, Avg loss: 2.160180 \n","\n","Epoch 2\n","-------------------------------\n","loss: 2.173233 [64/60000]\n","loss: 2.161397 [6464/60000]\n","loss: 2.110194 [12864/60000]\n","loss: 2.121940 [19264/60000]\n","loss: 2.075234 [25664/60000]\n","loss: 2.028837 [32064/60000]\n","loss: 2.042359 [38464/60000]\n","loss: 1.975331 [44864/60000]\n","loss: 1.987012 [51264/60000]\n","loss: 1.905722 [57664/60000]\n","Test Error: \n"," Accuracy: 53.3%, Avg loss: 1.901927 \n","\n","Epoch 3\n","-------------------------------\n","loss: 1.938120 [64/60000]\n","loss: 1.904198 [6464/60000]\n","loss: 1.796592 [12864/60000]\n","loss: 1.829580 [19264/60000]\n","loss: 1.728788 [25664/60000]\n","loss: 1.687254 [32064/60000]\n","loss: 1.698063 [38464/60000]\n","loss: 1.605114 [44864/60000]\n","loss: 1.637970 [51264/60000]\n","loss: 1.522627 [57664/60000]\n","Test Error: \n"," Accuracy: 57.8%, Avg loss: 1.538641 \n","\n","Epoch 4\n","-------------------------------\n","loss: 1.606139 [64/60000]\n","loss: 1.565687 [6464/60000]\n","loss: 1.423162 [12864/60000]\n","loss: 1.492030 [19264/60000]\n","loss: 1.380350 [25664/60000]\n","loss: 1.377270 [32064/60000]\n","loss: 1.383176 [38464/60000]\n","loss: 1.311717 [44864/60000]\n","loss: 1.354008 [51264/60000]\n","loss: 1.246207 [57664/60000]\n","Test Error: \n"," Accuracy: 61.9%, Avg loss: 1.270692 \n","\n","Epoch 5\n","-------------------------------\n","loss: 1.347705 [64/60000]\n","loss: 1.324692 [6464/60000]\n","loss: 1.163886 [12864/60000]\n","loss: 1.267711 [19264/60000]\n","loss: 1.147901 [25664/60000]\n","loss: 1.175272 [32064/60000]\n","loss: 1.186932 [38464/60000]\n","loss: 1.129319 [44864/60000]\n","loss: 1.173399 [51264/60000]\n","loss: 1.085037 [57664/60000]\n","Test Error: \n"," Accuracy: 64.1%, Avg loss: 1.103088 \n","\n","Epoch 6\n","-------------------------------\n","loss: 1.173418 [64/60000]\n","loss: 1.172174 [6464/60000]\n","loss: 0.993041 [12864/60000]\n","loss: 1.126651 [19264/60000]\n","loss: 1.001710 [25664/60000]\n","loss: 1.037987 [32064/60000]\n","loss: 1.064283 [38464/60000]\n","loss: 1.012142 [44864/60000]\n","loss: 1.055185 [51264/60000]\n","loss: 0.983211 [57664/60000]\n","Test Error: \n"," Accuracy: 65.4%, Avg loss: 0.994090 \n","\n","Epoch 7\n","-------------------------------\n","loss: 1.051643 [64/60000]\n","loss: 1.072702 [6464/60000]\n","loss: 0.875295 [12864/60000]\n","loss: 1.031809 [19264/60000]\n","loss: 0.907998 [25664/60000]\n","loss: 0.940280 [32064/60000]\n","loss: 0.982918 [38464/60000]\n","loss: 0.935088 [44864/60000]\n","loss: 0.972350 [51264/60000]\n","loss: 0.914349 [57664/60000]\n","Test Error: \n"," Accuracy: 67.2%, Avg loss: 0.919109 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.961600 [64/60000]\n","loss: 1.003206 [6464/60000]\n","loss: 0.790320 [12864/60000]\n","loss: 0.963865 [19264/60000]\n","loss: 0.844602 [25664/60000]\n","loss: 0.868166 [32064/60000]\n","loss: 0.925041 [38464/60000]\n","loss: 0.882741 [44864/60000]\n","loss: 0.911933 [51264/60000]\n","loss: 0.864768 [57664/60000]\n","Test Error: \n"," Accuracy: 68.3%, Avg loss: 0.864777 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.892220 [64/60000]\n","loss: 0.951108 [6464/60000]\n","loss: 0.726785 [12864/60000]\n","loss: 0.913066 [19264/60000]\n","loss: 0.799008 [25664/60000]\n","loss: 0.813690 [32064/60000]\n","loss: 0.881132 [38464/60000]\n","loss: 0.845929 [44864/60000]\n","loss: 0.866671 [51264/60000]\n","loss: 0.826628 [57664/60000]\n","Test Error: \n"," Accuracy: 69.7%, Avg loss: 0.823478 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.836456 [64/60000]\n","loss: 0.909622 [6464/60000]\n","loss: 0.677524 [12864/60000]\n","loss: 0.873853 [19264/60000]\n","loss: 0.764072 [25664/60000]\n","loss: 0.771590 [32064/60000]\n","loss: 0.845958 [38464/60000]\n","loss: 0.818729 [44864/60000]\n","loss: 0.831482 [51264/60000]\n","loss: 0.795991 [57664/60000]\n","Test Error: \n"," Accuracy: 70.9%, Avg loss: 0.790767 \n","\n","Done!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"X2uou4H6i0N7"},"execution_count":null,"outputs":[]}]}